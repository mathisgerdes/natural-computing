{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2b3d2ba-6854-4333-a2e4-f2d11f95ce45",
   "metadata": {},
   "source": [
    "# Natural computing \n",
    "\n",
    "According to [Wikipedia](https://en.wikipedia.org/wiki/Natural_computing), any method which:\n",
    "1. takes inspiration from nature to solve problems\n",
    "2. synthesize natural phenomena\n",
    "3. employ natural materials to compute\n",
    "\n",
    "Here, mainly interested in the first kind. The following is an incomplete list of examples.\n",
    "\n",
    "### 1. Inspired by nature\n",
    "Algorithms and paradigms for computation.\n",
    "- artificial neural networks\n",
    "- evolutionary algorithms\n",
    "- swarm intelligence\n",
    "- artificial immune systems\n",
    "- amorphous computing\n",
    "\n",
    "### 2. Synthesize natural phenomena\n",
    "Aim to understand underlying computational principles of natural phenomena.\n",
    "- artificial life\n",
    "- computational neuroscience\n",
    "- synthetic biology\n",
    "\n",
    "### 3. Natural materials to compute\n",
    "Instantiate desired computations (efficiently) using properties of physical matter.\n",
    "- analog computers\n",
    "- quantum computing\n",
    "- smart matter\n",
    "- DNA computing, molecular computing\n",
    "- membrane computing, (actual) neural networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b02fa65-d344-4504-a588-47465c8cb5ea",
   "metadata": {},
   "source": [
    "# General problem solving\n",
    "**Utility hypothesis:** We can encode every problem using a utility function such that the solution is given by its maxima.\n",
    "\n",
    "*Roughly synonymous (up to sign):* Objective function, utility function, loss function, energy, cost function, fitness, ...\n",
    "\n",
    "$\\quad\\rightarrow$ Optimization / minimization / maximization\n",
    "\n",
    "In effect, we are interested in solving a search problem specified in terms of an objective function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dece7f29-c880-40c7-bf4c-b3c1584d2673",
   "metadata": {},
   "source": [
    "## How to find a solution?\n",
    "$\\quad\\uparrow$ *specific, exact*\n",
    "\n",
    "- direct calculation\n",
    "- iterative solution, \"divide and conquer\"\n",
    "- heuristics, meta-heuristic algorithms\n",
    "- random guessing\n",
    "\n",
    "$\\quad\\downarrow$ *general, sufficient*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b606a13-3e2e-44c8-8c9b-0b2d2c5b94b8",
   "metadata": {},
   "source": [
    "## Heuristics\n",
    "From [Wikipedia]():\n",
    "> A heuristic function, also simply called a heuristic, is a function that ranks alternatives in search algorithms at each branching step based on available information to decide which branch to follow. For example, it may approximate the exact solution.\n",
    "\n",
    "An example can be found in the [A* path search algorithm](https://en.wikipedia.org/wiki/A*_search_algorithm).\n",
    "Given a graph of connections between nodes, its goal is to find a path connecting a start node and an end node such that some cost function (e.g. distance) is minamal.\n",
    "This is done by maintaining a tree of incomplete paths originating at the start node.\n",
    "The tree is iteratively expanded by choosing a next node $n$ such that the expected cost $f(n)$ is minimized:\n",
    "$$f(n) = g(n) + h(n)\\,.$$\n",
    "\n",
    "Here, $g(n)$ is the (exact) cost of the path from the origin to $n$ so far.\n",
    "$h(n)$ is an esitmate or *heuristic* for the cost to go from $n$ to the goal node.\n",
    "An example of this is the straight-line distance, where $g$ is the distance following a network of streets (given by the graph).\n",
    "\n",
    "![Straight-line heuristic](https://upload.wikimedia.org/wikipedia/commons/2/2c/Heuristic-straight-line-distance.svg)\n",
    "\n",
    "Source: [wikimedia](https://upload.wikimedia.org/wikipedia/commons/2/2c/Heuristic-straight-line-distance.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4e06fc-54d7-462f-9ce7-79bfa72797a8",
   "metadata": {},
   "source": [
    "## Metaheuristics\n",
    "Large class of optimization algorithms which aim to find (approximate) solutions by sampling a subset of the search space where complete enumeration is infeasible.\n",
    "- Usually have a number of (hyper-) parameters which affect performance\n",
    "- Combined e.g. genetic algorithms with machine learning (hybridization)\n",
    "- General tradeoff between **exploitation** (greedy, local optima) and **exploration** (escape local optima, larger search space)\n",
    "- Convergnece to optimal solution not guaranteed\n",
    "\n",
    "![Overview of metaheuristics](https://upload.wikimedia.org/wikipedia/commons/c/c3/Metaheuristics_classification.svg)\n",
    "\n",
    "Source: [wikimedia](https://upload.wikimedia.org/wikipedia/commons/c/c3/Metaheuristics_classification.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60760e7-f34b-41e0-bff1-21196990bce0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# No free lunch theorem\n",
    "\n",
    "We have a finite solution space $X$ and a partially ordered cost-value set $Y$ (for example $\\mathbb{R}$).\n",
    "The set of objective functions $f: X \\rightarrow Y$ is then $Y^X$.\n",
    "\n",
    "The search algorithm only knows the objective function on the samples evaluated so far.\n",
    "These data are summarized in the sequence\n",
    "$$d_m = [(x_0, f_0), (x_1, f_1), \\ldots, (x_m, f_m)] \\,.$$\n",
    "An optimization algorithm is then defined by a function\n",
    "$$A(d_m) = x_{m+1} \\,.$$\n",
    "We further assume that the algorithm is non-repeating.\n",
    "\n",
    "Finally, we assume that the performance measure $c\\in\\mathbb{R}$ of the algorithm depends only on the observed cost values:\n",
    "$$c(f, m, A) = c(\\{f_i\\}) \\stackrel{e.g.}{=} min_{0 < i \\leq m} f_i\\,.$$\n",
    "\n",
    "Original NFL theorem:\n",
    "For any two algorithms $A$, $B$, any performance measure $c$, any $m$ and any $k\\in\\mathbb{R}$,\n",
    "\n",
    "$$\\sum_{f \\in Y^X} \\delta(k, c(f, m, A)) = \\sum_{f \\in Y^X} \\delta(k, c(f, m, B))\\,.$$\n",
    "\n",
    "In other words, for any $f_a \\in Y^X$ there is another $f_b \\in Y^X$ such that $c(f_a, m, A) = c(f_b, m, B)$.\n",
    "\n",
    "This can be generalized to subsets $F \\subset Y^X$, under the condition that $F$ is closed under permutations of $X$.\n",
    "There is also a [generalization](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.158.9715&rep=rep1&type=pdf) to non-uniform distributions over $F$ (the distribution is required to be equal for two functions $f$ connected by a permutation of the space $X$).\n",
    "\n",
    "Implications\n",
    "- Averaged over all possible discrete cost functions, no algorithm is better than any other\n",
    "- Without prior knowledge on $f$, information of past samples cannot predict value on future samples\n",
    "- No meaningful comparison without reference to specific problem"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scientific Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
